{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzs89KPPde/g67jLoTJO7f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arsalanyaghoobi/BertForTokenClassification-using-keras/blob/main/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI_m8AJIrfHy"
      },
      "source": [
        "Name Entity recognition `build knowledge from unstructured text data`. It parses important information form the text like `email address, phone number, degree titles, location names, organizations, time and etc`.\n",
        "\n",
        "\n",
        "Named Entity Recognition is the task of determining a set of entities in the sentence.\n",
        "In other words, each word is classified to one of the predefined entities.\n",
        "BERT models can be used to solve the NER task by adding a softmax layer after the last embedding layer. Since the embeddings generated from BERT already hold information about the realtion between the word and the other words of the sentence and thus are suitable for the NER task since the entities can hugely depend on the context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP6aHGLfV_Sc"
      },
      "source": [
        "https://androidkt.com/name-entity-recognition-with-bert-in-tensorflow/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z_nFCZXh-yzW",
        "outputId": "a407a3c6-96c3-434a-d430-56b33bf62c51"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb39_XG4u1Q7"
      },
      "source": [
        "Transfomers library contains some state-of-the-art pre-trained models for Natural Language Processing (NLP) like BERT, GPT, XLNet … etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0oK1sYgvt9m"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW0GZml0wXQB"
      },
      "source": [
        "Pandas is the most popular data manipulation package in Python.Bear in mind that Panda is different from Pandas ; and Pandas is our case.\n",
        "\n",
        "NumPy aims to provide an array object that is up to 50x faster that traditional Python lists. it provides a lot of supporting functions that make working with ndarray very easy. Arrays are very frequently used in data science, where speed and resources are very important.An array is a data structure that stores values of same data type. In Python, this is the main difference between arrays and lists. While python lists can contain values corresponding to different data types, arrays in python can only contain values corresponding to same data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsgIDfu6tUTW"
      },
      "source": [
        "#from google.colab import files\n",
        "#files.upload()\n"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Zn-Pwue0u4CJ",
        "outputId": "34d0b41b-2a12-49ab-db0d-2a84ef82593f"
      },
      "source": [
        "df_data = pd.read_csv(\"/content/ner_dataset.csv\", error_bad_lines=False,sep=\",\",encoding=\"latin1\").fillna(method='ffill')\n",
        "df_data.head"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of               Sentence #           Word  POS Tag\n",
              "0            Sentence: 1      Thousands  NNS   O\n",
              "1            Sentence: 1             of   IN   O\n",
              "2            Sentence: 1  demonstrators  NNS   O\n",
              "3            Sentence: 1           have  VBP   O\n",
              "4            Sentence: 1        marched  VBN   O\n",
              "...                  ...            ...  ...  ..\n",
              "1048570  Sentence: 47959           they  PRP   O\n",
              "1048571  Sentence: 47959      responded  VBD   O\n",
              "1048572  Sentence: 47959             to   TO   O\n",
              "1048573  Sentence: 47959            the   DT   O\n",
              "1048574  Sentence: 47959         attack   NN   O\n",
              "\n",
              "[1048575 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leg0_eoYKKrr"
      },
      "source": [
        "Now we split the dataset to use 20% to validate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "thzf4RrmKLjn",
        "outputId": "e0b02794-7175-4b2e-d119-b037e6d7847d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test = train_test_split(df_data,test_size = 0.20, shuffle=False)\n",
        "print(x_train)\n",
        "#print(x_test)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             Sentence #           Word  POS Tag\n",
            "0           Sentence: 1      Thousands  NNS   O\n",
            "1           Sentence: 1             of   IN   O\n",
            "2           Sentence: 1  demonstrators  NNS   O\n",
            "3           Sentence: 1           have  VBP   O\n",
            "4           Sentence: 1        marched  VBN   O\n",
            "...                 ...            ...  ...  ..\n",
            "838855  Sentence: 38346           raft   NN   O\n",
            "838856  Sentence: 38346           from   IN   O\n",
            "838857  Sentence: 38346            one   CD   O\n",
            "838858  Sentence: 38346             of   IN   O\n",
            "838859  Sentence: 38346            the   DT   O\n",
            "\n",
            "[838860 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YauW5xKKgvv"
      },
      "source": [
        "Sklearn,  features various classification, regression and clustering algorithms.\n",
        "Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGXMKY1vSfQz"
      },
      "source": [
        "drawing stacked bar plot using \n",
        "`data.groupby('feature')['label'].value_counts()`\n",
        "\n",
        "pandas objects can be split on any of their axes. The abstract definition of grouping is to provide a mapping of labels to group names. To create a GroupBy object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4afHeim9wex"
      },
      "source": [
        "agg_func = lambda s: [[w,t] for w,t in zip(s[\"Word\"].values.tolist(),s[\"Tag\"].values.tolist())]    #What is agg function?read the text above.\n",
        "#agg_func = lambda w,t: [[w,t] for w,t in zip([\"Word\"].values.tolist(),[\"Tag\"].values.tolist())]      #I changed from zip to ner_dataset and I got error in the cell\n",
        "                                                                                                     #below; where is Zip?                                                    "
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePV6EbzV9C1f"
      },
      "source": [
        "x_train_grouped = x_train.groupby(\"Sentence #\").apply(agg_func)  # make list of 838860 row* 3 column including Tag, Name and Sentences.\n",
        "x_test_grouped = x_test.groupby(\"Sentence #\").apply(agg_func)    "
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hx62nph_bn_A",
        "outputId": "0c842c02-6cb1-4e06-ae01-26406cad67ae"
      },
      "source": [
        "print(x_train_grouped)"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence #\n",
            "Sentence: 1        [[Thousands, O], [of, O], [demonstrators, O], ...\n",
            "Sentence: 10       [[Iranian, B-gpe], [officials, O], [say, O], [...\n",
            "Sentence: 100      [[Helicopter, O], [gunships, O], [Saturday, B-...\n",
            "Sentence: 1000     [[They, O], [left, O], [after, O], [a, O], [te...\n",
            "Sentence: 10000    [[U.N., B-geo], [relief, O], [coordinator, O],...\n",
            "                                         ...                        \n",
            "Sentence: 9995     [[Opposition, O], [leader, O], [Mir, O], [Hoss...\n",
            "Sentence: 9996     [[On, O], [Thursday, B-tim], [,, O], [Iranian,...\n",
            "Sentence: 9997     [[Following, O], [Iran, B-geo], ['s, O], [disp...\n",
            "Sentence: 9998     [[Since, O], [then, O], [,, O], [authorities, ...\n",
            "Sentence: 9999     [[The, O], [United, B-org], [Nations, I-org], ...\n",
            "Length: 38346, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7m0mFIA9DVd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "578e11bc-3641-4d53-8d8b-fd68c72e11e6"
      },
      "source": [
        "x_train_sentences = [[s[0] for s in sent] for sent in x_train_grouped.values]       #seperation of words from tags in both training and testing dataset\n",
        "x_test_sentences = [[s[0] for s in sent] for sent in x_test_grouped.values]         #s[0]= Words , s[1]=Tag\n",
        "print(x_train_sentences[10])                                                        #what is sent??????????"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', 'Beirut', ',', 'a', 'string', 'of', 'officials', 'voiced', 'their', 'anger', ',', 'while', 'at', 'the', 'United', 'Nations', 'summit', 'in', 'New', 'York', ',', 'Prime', 'Minister', 'Fouad', 'Siniora', 'said', 'the', 'Lebanese', 'people', 'are', 'resolute', 'in', 'preventing', 'such', 'attempts', 'from', 'destroying', 'their', 'spirit', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNjVr2HJ9DY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "081fed7e-a22d-4ed7-d3cc-8e23463276b5"
      },
      "source": [
        "x_train_tags = [[t[1] for t in tag] for tag in x_train_grouped.values]            #seperation of words from tags in both training and testing dataset\n",
        "x_test_tags = [[t[1] for t in tag] for tag in x_test_grouped.values]              #s[1]? or t[1]?\n",
        "print(x_train_tags[10])"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'I-org', 'O', 'O', 'B-geo', 'I-geo', 'O', 'B-per', 'O', 'B-per', 'I-per', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd8BnOoml5ux"
      },
      "source": [
        "MAX_LENGTH = 128\n",
        "BERT_MODEL = 'bert-base-cased'\n",
        "BATCH_SIZE = 32\n",
        "pad_token = 0\n",
        "#pad_token_segment_id = 0    #??????\n",
        "#sequence_a_segment_id = 0   #??????"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbA7j7e3rEEM"
      },
      "source": [
        "from transformers import(\n",
        "    #TF2_WEIGHTS_NAME,\n",
        "    BertConfig,\n",
        "    BertTokenizer,\n",
        "    TFBertForTokenClassification,\n",
        "    #create_optimizer\n",
        "    )\n",
        "tokenizer_class = BertTokenizer\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(BERT_MODEL,do_lower_case=False)"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nD0B4Zh5jJb"
      },
      "source": [
        "TFBertForTokenClassification is a fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. We load the pre-trained “bert-base-cased” model and provide the number of possible labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9bSqGsT1moA"
      },
      "source": [
        "Creat a Model:\n",
        "\n",
        "`for any Bert model we need an input and a label`;NER is the multi-class classification problem where the words are our input and tags are our labels.\n",
        "\n",
        "The base class PretrainedConfig `implements the common methods for loading/saving a configuration either from a local file or directory`, or from a pretrained model configuration provided by the library (downloaded from HuggingFace’s AWS S3 repository)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AabXbQcJ15N5",
        "outputId": "b857b404-2391-40c5-fc49-74220e401e2e"
      },
      "source": [
        "MODEL_CLASSES = {'bert':  (BertConfig, TFBertForTokenClassification, BertTokenizer)}\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
        "config = config_class.from_pretrained(BERT_MODEL,num_labels=50000)  #what is the number of labels?\n",
        "\n",
        "model =model_class.from_pretrained(\n",
        "    BERT_MODEL,\n",
        "    from_pt= bool('.bin'in BERT_MODEL),\n",
        "    config = config\n",
        ")"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertForTokenClassification: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['dropout_493', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xcUBtg3hPWNm",
        "outputId": "8b0aa0a1-c50d-4b75-8384-ab49b06abb61"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_token_classification_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  108310272 \n",
            "_________________________________________________________________\n",
            "dropout_493 (Dropout)        multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  38450000  \n",
            "=================================================================\n",
            "Total params: 146,760,272\n",
            "Trainable params: 146,760,272\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFZHqUdBaAa1"
      },
      "source": [
        "The tensor y_true is the true data (or target, ground truth) you pass to the fit method.\n",
        "It's a conversion of the numpy array y_train into a tensor.\n",
        "\n",
        "1)Two very easy ways to find the shape of y_true are:\n",
        "check your true/target data: print(Y_train.shape)\n",
        "check your model.summary() and see the last output; if your last layer outputs (None, 1), the shape of y_true is (batch, 1). If the last layer outputs (None, 200,200, 3), then y_true will be (batch, 200,200,3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYKFJNaUhXqz"
      },
      "source": [
        "model.layers[-1].activation = tf.keras.activations.softmax"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoVRecN608_R"
      },
      "source": [
        "Prepare Input\n",
        "\n",
        "Before we can start fine-tuning the model, we have to prepare the data set for use with BERT.We need to set the text into 3 kinds of embeddings:\n",
        "\n",
        "1)Token Embedding:\n",
        "In order to make token embedding, we need to map the word token into the id.\n",
        "\n",
        "2)In order to make mask word embedding, we need to use 1 to indicate the real toke and 0 to indicate to pad token.\n",
        "\n",
        "3)Where “token_type_ids” are used to indicate whether this is the first # sequence or the second sequence.\n",
        "\n",
        "Next, we cut and pad the token and label sequences to our desired length.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktuXw7GHPWCy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4361830a-6c36-4dd6-9752-82a44d60df19"
      },
      "source": [
        "tag_list=df_data.Tag.unique()   #tag_list will help you to create your label_map \n",
        "print(tag_list)\n",
        "\n",
        "label_map = {label: i for i, label in enumerate(tag_list)}      # we need to define number of nodes for the very last layers; number of nodes equal to number of out put Entity.\n",
        "#It is used for defining lable_ids   \n",
        "\n",
        "num_labels = len(tag_list) + 1    #you will use number of labels for configuraion stage.\n",
        "num_labels                                                          #????????"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O' 'B-geo' 'B-gpe' 'B-per' 'I-geo' 'B-org' 'I-org' 'B-tim' 'B-art'\n",
            " 'I-art' 'I-per' 'I-gpe' 'I-tim' 'B-nat' 'B-eve' 'I-eve' 'I-nat']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "632WOH-7Zbgt"
      },
      "source": [
        "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEaQs9uFZblp"
      },
      "source": [
        "config = config_class.from_pretrained(BERT_MODEL,num_labels=num_labels)   "
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUScSwnWbKMU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c47905e5-838a-4e27-a86d-cbe0e41bf886"
      },
      "source": [
        "model = model_class.from_pretrained(\n",
        "                BERT_MODEL,\n",
        "                from_pt=bool(\".bin\" in BERT_MODEL),\n",
        "                config=config)"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertForTokenClassification: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['dropout_531', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCSNlfzucPSF"
      },
      "source": [
        " model.layers[-1].activation = tf.keras.activations.softmax     #defining he very last layer activation"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X-JzYv2C86L1",
        "outputId": "2b07f836-f5ea-490d-fe2e-44a1717ea760"
      },
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEU86-Es9D1O"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_seq_length =128\n",
        "\n",
        "def convert_to_input(sentences,tags):\n",
        "  input_id_list,attention_mask_list,token_type_id_list=[],[],[]\n",
        "  label_id_list=[]   #did it differe to have this among the upper items?\n",
        "  \n",
        "  for x,y in tqdm(zip(sentences,tags), total=len(tags)): #?????????\n",
        "  \n",
        "    tokens = []\n",
        "    label_ids = []\n",
        "\n",
        "    for word, label in zip(x, y):\n",
        "      word_tokens = tokenizer.tokenize(word)\n",
        "      tokens.extend(word_tokens)  #Python extend() is an inbuilt function that adds the specified list \n",
        "                                  #elements (or any iterable) to the end of the current list. \n",
        "                                  #The extend() method extends the list by adding all items of the list (passed as an argument) to an end.\n",
        "     # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "      label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "  \n",
        "    special_tokens_count =  2\n",
        "    if len(tokens) > max_seq_length - special_tokens_count:\n",
        "      tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
        "      label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
        "\n",
        "    label_ids = [pad_token_label_id]+label_ids+[pad_token_label_id]\n",
        "    inputs = tokenizer.encode_plus(tokens,add_special_tokens=True, max_length=max_seq_length)\n",
        "\n",
        "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "    attention_masks = [1] * len(input_ids)\n",
        "\n",
        "    attention_mask_list.append(attention_masks)\n",
        "    input_id_list.append(input_ids)\n",
        "    token_type_id_list.append(token_type_ids)\n",
        "\n",
        "    label_id_list.append(label_ids)\n",
        "\n",
        "  return input_id_list,token_type_id_list,attention_mask_list,label_id_list\n"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dhFVtDyZKSz"
      },
      "source": [
        "pad_token_label_id = 0"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8bvA85FQ9bV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4b383fd7-566a-4194-ce4b-1e4bc5bd8826"
      },
      "source": [
        "input_ids_train,token_ids_train,attention_masks_train,label_ids_train=convert_to_input(x_train_sentences,x_train_tags)"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/38346 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 38346/38346 [00:42<00:00, 905.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1D5KcPy37laI",
        "outputId": "a0f4e6c8-726b-4f0c-d4d7-2105b5749dba"
      },
      "source": [
        "input_ids_test,token_ids_test,attention_masks_test,label_ids_test=convert_to_input(x_test_sentences,x_test_tags)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9614/9614 [00:10<00:00, 932.42it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZP07bWT9D35"
      },
      "source": [
        "input_ids_train = pad_sequences(input_ids_train,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "token_ids_train = pad_sequences(token_ids_train,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "attention_masks_train = pad_sequences(attention_masks_train,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "label_ids_train = pad_sequences(label_ids_train,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnxt2qII9D9l"
      },
      "source": [
        "input_ids_test = pad_sequences(input_ids_test,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "token_ids_test = pad_sequences(token_ids_test,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "attention_masks_test = pad_sequences(attention_masks_test,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "label_ids_test = pad_sequences(label_ids_test,maxlen=max_seq_length,dtype=\"long\",truncating=\"post\",padding=\"post\")"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-qMhDlM9D6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bb624c3c-ab43-482b-bbc4-8c6812238056"
      },
      "source": [
        "np.shape(input_ids_train),np.shape(token_ids_train),np.shape(attention_masks_train),np.shape(label_ids_train),"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((38346, 128), (38346, 128), (38346, 128), (38346, 128))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP9_IbMn9EC2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a0b850eb-a444-4218-eb5a-c7b1072dcd5f"
      },
      "source": [
        "np.shape(input_ids_test),np.shape(token_ids_test),np.shape(attention_masks_test),np.shape(label_ids_test),"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9614, 128), (9614, 128), (9614, 128), (9614, 128))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHRTD6529EFz"
      },
      "source": [
        "def example_to_features(input_ids,attention_masks,token_type_ids,y):\n",
        "  return {\"input_ids\": input_ids,\n",
        "          \"attention_mask\": attention_masks,\n",
        "          \"token_type_ids\": token_type_ids},y\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((input_ids_train,attention_masks_train,token_ids_train,label_ids_train)).map(example_to_features).shuffle(1000).batch(32).repeat(5)\n",
        "\n",
        "\n",
        "test_ds=tf.data.Dataset.from_tensor_slices((input_ids_test,attention_masks_test,token_ids_test,label_ids_test)).map(example_to_features).batch(1)\n"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLc3V3E8wokD"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_seq_length =128"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3tuiJrsQ1Qi2",
        "outputId": "549d7487-5da8-48a3-d06b-23cd634d40b7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_token_classification_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  108310272 \n",
            "_________________________________________________________________\n",
            "dropout_531 (Dropout)        multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  13842     \n",
            "=================================================================\n",
            "Total params: 108,324,114\n",
            "Trainable params: 108,324,114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shc1FsOu9k6v"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HTS8vUa9k9i"
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARVTs-no9lCh"
      },
      "source": [
        "history = model.fit(train_ds, epochs=3, validation_data=test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}